{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "import keras.src.utils as image\n",
    "from keras.src import layers \n",
    "from keras.src.applications.vgg16 import VGG16\n",
    "from keras.src.models import Model\n",
    "from keras.api.saving import load_model\n",
    "from keras.src.legacy.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "def predict_apple(img_path):\n",
    "  \"\"\"\n",
    "  Predicts whether an image contains an apple using a pre-trained VGG16 model.\n",
    "\n",
    "  Args:\n",
    "      img_path (str): Path to the image file.\n",
    "\n",
    "  Returns:\n",
    "      bool: True if the image is predicted to contain an apple, False otherwise.\n",
    "  \"\"\"\n",
    "\n",
    "  # # Load the pre-trained VGG16 model without the top layers (freeze weights)\n",
    "  # base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "  # for layer in base_model.layers:\n",
    "  #   layer.trainable = False\n",
    "\n",
    "  # # Add custom layers for classification\n",
    "  # x = base_model.output\n",
    "  # x = layers.Flatten()(x)\n",
    "  # x = layers.Dense(512, activation='relu')(x)\n",
    "  # x = layers.Dropout(0.5)(x)\n",
    "  # predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "  # # Create the final model\n",
    "  # model = Model(inputs=base_model.input, outputs=predictions)\n",
    "  # # Create a new model instance\n",
    "  # model.load_weights('apple_classifier.weights.h5')\n",
    "\n",
    "  model = load_model('apple_classifier.keras')\n",
    "  \n",
    "  # Load the model weights (assuming they are saved after training)\n",
    "  # model =  load_weights('apple_classifier.keras','apple_classifier.keras', skip_mismatch=False)  # Replace with your weights file path\n",
    "\n",
    "  # Preprocess the image\n",
    "  img = image.load_img(img_path, target_size=(224, 224))\n",
    "  x = image.img_to_array(img)\n",
    "  x = x / 255.0  # Rescale to [0, 1]\n",
    "  # Add a batch dimension\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "  # Make prediction\n",
    "  prediction = model.predict(x)[0][0]\n",
    "  apple_probability = prediction\n",
    "\n",
    "  # Set a threshold for classification (adjust as needed)\n",
    "  threshold = 0.5  # Consider image an apple if probability >= threshold\n",
    "  print('Apple probability:', apple_probability)\n",
    "  print('Apple probability:', model.class_indices)\n",
    "\n",
    "  return apple_probability >= threshold\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step\n",
      "Apple probability: 0.6176021\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'class_indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict_apple\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mman.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict_apple(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanz.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(predict_apple(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg1.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[1;32mIn[8], line 62\u001b[0m, in \u001b[0;36mpredict_apple\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m     60\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Consider image an apple if probability >= threshold\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApple probability:\u001b[39m\u001b[38;5;124m'\u001b[39m, apple_probability)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApple probability:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_indices\u001b[49m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m apple_probability \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'class_indices'"
     ]
    }
   ],
   "source": [
    "print(predict_apple('man.jpg'))\n",
    "print(predict_apple('manz.jpg'))\n",
    "print(predict_apple('img1.jpg'))\n",
    "print(predict_apple('img2.jpg'))\n",
    "print(predict_apple('img3.jpg'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
